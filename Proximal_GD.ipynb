{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import time as tm\n",
    "from numpy import linalg as Lalg\n",
    "\n",
    "# SUBMIT YOUR CODE AS A SINGLE PYTHON (.PY) FILE INSIDE A ZIP ARCHIVE\n",
    "# THE NAME OF THE PYTHON FILE MUST BE SUBMIT.PY\n",
    "# DO NOT INCLUDE PACKAGES LIKE SKLEARN, SCIPY, KERAS ETC IN YOUR CODE\n",
    "# THE USE OF ANY MACHINE LEARNING LIBRARIES FOR WHATEVER REASON WILL RESULT IN A STRAIGHT ZERO\n",
    "# THIS IS BECAUSE THESE PACKAGES CONTAIN SOLVERS WHICH MAKE THIS ASSIGNMENT TRIVIAL\n",
    "\n",
    "# DO NOT CHANGE THE NAME OF THE METHOD \"solver\" BELOW. THIS ACTS AS THE MAIN METHOD AND\n",
    "# WILL BE INVOKED BY THE EVALUATION SCRIPT. CHANGING THIS NAME WILL CAUSE EVALUATION FAILURES\n",
    "\n",
    "# You may define any new functions, variables, classes here\n",
    "# For example, functions to calculate next coordinate or step length\n",
    "\n",
    "################################\n",
    "# Non Editable Region Starting #\n",
    "################################\n",
    "def solver( X, y, timeout, spacing ):\n",
    "\t(n, d) = X.shape\n",
    "\tt = 0\n",
    "\ttotTime = 0\n",
    "\t\n",
    "\t# w is the model vector and will get returned once timeout happens\n",
    "\tw = np.zeros( (d,) )\n",
    "\ttic = tm.perf_counter()\n",
    "################################\n",
    "#  Non Editable Region Ending  #\n",
    "################################\n",
    "    \n",
    "\t# You may reinitialize w to your liking here\n",
    "\t# You may also define new variables here e.g. step_length, mini-batch size etc\n",
    "\tR = 3\n",
    "\talpha = 36\n",
    "\tl = 1/R\n",
    "\t#w_ = np.zeros((d,))\n",
    "\tmyloss = 0\n",
    "    \n",
    "\tdef soft_thres(W, hyper):\n",
    "\t\tfor i in range(0,len(W)):\n",
    "\t\t\tif (W[i] > hyper):\n",
    "\t\t\t\tW[i] = W[i]-hyper\n",
    "\t\t\telif (W[i] < hyper):\n",
    "\t\t\t\tW[i] = W[i] + hyper\n",
    "\t\t\telse:\n",
    "\t\t\t\tW[i] = 0\n",
    "\t\treturn W\n",
    "\n",
    "################################\n",
    "# Non Editable Region Starting #\n",
    "################################\n",
    "\twhile True:\n",
    "\t\tt = t + 1\n",
    "\t\tif t % spacing == 0:\n",
    "\t\t\ttoc = tm.perf_counter()\n",
    "\t\t\ttotTime = totTime + (toc - tic)\n",
    "\t\t\tif totTime > timeout:\n",
    "\t\t\t\treturn (w, totTime) \n",
    "\t\t\telse:\n",
    "\t\t\t\ttic = tm.perf_counter()\n",
    "################################\n",
    "#  Non Editable Region Ending  #\n",
    "################################\n",
    "\n",
    "\t\t# Write all code to perform your method updates here within the infinite while loop\n",
    "\t\t# The infinite loop will terminate once timeout is reached\n",
    "\t\t# Do not try to bypass the timer check e.g. by using continue\n",
    "\t\t# It is very easy for us to detect such bypasses which will be strictly penalized\n",
    "\t\t\n",
    "\t\t# Please note that once timeout is reached, the code will simply return w\n",
    "\t\t# Thus, if you wish to return the average model (as is sometimes done for GD),\n",
    "\t\t# you need to make sure that w stores the average at all times\n",
    "\t\t# One way to do so is to define a \"running\" variable w_run\n",
    "\t\t# Make all GD updates to w_run e.g. w_run = w_run - step * delw\n",
    "\t\t# Then use a running average formula to update w\n",
    "\t\t# w = (w * (t-1) + w_run)/t\n",
    "\t\t# This way, w will always store the average and can be returned at any time\n",
    "\t\t# In this scheme, w plays the role of the \"cumulative\" variable in the course module optLib\n",
    "\t\t# w_run on the other hand, plays the role of the \"theta\" variable in the course module optLib\n",
    "        \n",
    "\t\tl = 1/R\n",
    "\t\tdiff = (X.dot(w)-y)\n",
    "\t\tw_ = w - l*(X.T).dot(diff)\n",
    "        \n",
    "        #Soft-Thresholding_call\n",
    "\t\tw_ = soft_thres(w_, l)\n",
    "        \n",
    "        #Hyperparameter Tuning step\n",
    "\t\twhile (Lalg.norm(y-X.dot(w_),2)**2 > Lalg.norm(y-X.dot(w),2)**2 + (np.transpose(w_-w)).dot((( np.transpose(X) ).dot(X.dot(w)-y))) + (R/2)*(Lalg.norm((w_-w),2) ) ):\n",
    "\t\t\tR = alpha*R\n",
    "\t\t\tl = 1/R\n",
    "        \n",
    "\t\tw = w_\n",
    "        #This is the loss function\n",
    "\t\tmyloss = Lalg.norm(y-X.dot(w_), 2)**2 + Lalg.norm(w_ ,1)\n",
    "\t\t#print (myloss)\n",
    "        \n",
    "\tprint (myloss)    \n",
    "\treturn (w, totTime) # This return statement will never be reached \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out how much loss is the learnt model incurring?\n",
    "def getObjValue( X, y, wHat ):\n",
    "\tlassoLoss = np.linalg.norm( wHat, 1 ) + pow( np.linalg.norm( X.dot( wHat ) - y, 2 ), 2 )\n",
    "\treturn lassoLoss\n",
    "\n",
    "# Find out how far is the learnt model from the true one in terms of Euclidean distance\n",
    "def getModelError( wHat, wAst ):\n",
    "\treturn np.linalg.norm( wHat - wAst, 2 )\n",
    "\n",
    "# Force the learnt model to become sparse and then see how well it approximates the true model\n",
    "def getSupportError( wHat, wAst, k ):\n",
    "\t# Find the k coordinates where the true model has non-zero values\n",
    "\tidxAst = np.abs( wAst ).argsort()[::-1][:k]\n",
    "\t# Find the k coordinates with largest values (absolute terms) in the learnt model\n",
    "\tidxHat = np.abs( wHat ).argsort()[::-1][:k]\n",
    "\t\n",
    "\t# Set up indicator arrays to find the diff between the two\n",
    "\t# Could have used Python's set difference function here as well\n",
    "\ta = np.zeros_like( wAst )\n",
    "\ta[idxAst] = 1\n",
    "\tb = np.zeros_like( wAst )\n",
    "\tb[idxHat] = 1\n",
    "\treturn np.linalg.norm( a - b, 1 )//2\n",
    "\n",
    "# Z = np.loadtxt( \"train\" )\n",
    "# wAst = np.loadtxt( \"wAstTrain\" )\n",
    "Z = np.loadtxt( r\"H:\\CS 771\\assn1\\train\" )\n",
    "#print (Z.shape)\n",
    "wAst = np.loadtxt( r\"H:\\CS 771\\assn1\\wAstTrain\" )\n",
    "k = 20\n",
    "\n",
    "y = Z[:,0]\n",
    "X = Z[:,1:]\n",
    "\n",
    "# To avoid unlucky outcomes try running the code several times\n",
    "numTrials = 5\n",
    "\n",
    "# Try various timeouts - the timeouts are in seconds\n",
    "timeouts = np.array( [0.1, 1, 2, 5] )\n",
    "\n",
    "# Try checking for timeout every 10 iterations\n",
    "spacing = 10\n",
    "\n",
    "result = np.zeros( (len( timeouts ), 4) )\n",
    "\n",
    "for i in range(0, len( timeouts )):\n",
    "\tto = timeouts[i]\n",
    "\tavgObj = 0\n",
    "\tavgDist = 0\n",
    "\tavgSupp = 0\n",
    "\tavgTime = 0\n",
    "\tfor t in range( numTrials ):\n",
    "\t\t(w, totTime) = solver( X, y, to, spacing )\n",
    "\t\tavgObj = avgObj + getObjValue( X, y, w )\n",
    "\t\tavgDist = avgDist + getModelError( w, wAst )\n",
    "\t\tavgSupp = avgSupp + getSupportError( w, wAst, k )\n",
    "\t\tavgTime = avgTime + totTime\n",
    "\tresult[i, 0] = avgObj/numTrials\n",
    "\tresult[i, 1] = avgDist/numTrials\n",
    "\tresult[i, 2] = avgSupp/numTrials\n",
    "\tresult[i, 3] = avgTime/numTrials\n",
    "\n",
    "np.savetxt( \"result\", result, fmt = \"%.6f\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
